{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>XGBoost Fraud Detection using a highly unbalanced dataset from Kaggle</b><br>\n",
    "No feature engineering allowing for a concise notebook illustrating Hyperparameter Tuning & Training with AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Python Modules\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sagemaker import s3_input\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.predictor import json_deserializer\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.tuner import IntegerParameter\n",
    "from sagemaker.tuner import ContinuousParameter\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Unique Variables\n",
    "\n",
    "role             = 'AmazonSageMaker-ExecutionRole-20191005T164168'\n",
    "prefix           = 'xgboost'\n",
    "bucket           = 'creditcardfraud123'\n",
    "repo_name        = 'xgboost'\n",
    "repo_version     = 'latest'\n",
    "csv_filename     = 's3://' + bucket + '/creditcard.csv'\n",
    "endpoint_name    = 'creditcardfraudxgboost'\n",
    "sagemaker_client = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly Separate Train, Validation, and Test Datasets\n",
    "\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "model_data = df[df.columns.tolist()[-1:] + df.columns.tolist()[:-1]]\n",
    "\n",
    "train_data, validation_data, test_data = np.split(\n",
    "                                                  model_data.sample(frac=1),\n",
    "                                                  [\n",
    "                                                   int(0.7 * len(model_data)),\n",
    "                                                   int(0.9 * len(model_data))\n",
    "                                                  ]\n",
    "                                                 )\n",
    "\n",
    "train_data.to_csv(     'train.csv'     , header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv'          )).upload_file('train.csv'     )\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Fraud Count in Train Set = 284315.0\n",
      "Fraud Count in Train Set     = 492.0\n",
      "Class Weight Scale Factor    = 577.9\n"
     ]
    }
   ],
   "source": [
    "# Define Class Weights for Highly Unbalanced Fraud Data\n",
    "\n",
    "class_counts  = df['Class'].reset_index().groupby('Class').agg('count').values\n",
    "class_weights = float(class_counts[0]/float(class_counts[1]))\n",
    "\n",
    "print('Non-Fraud Count in Train Set = {}'.format(    float(class_counts[0])))\n",
    "print('Fraud Count in Train Set     = {}'.format(    float(class_counts[1])))\n",
    "print('Class Weight Scale Factor    = {:.1f}'.format(class_weights         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image.To use the newer image, please set 'repo_version'='0.90-1. For example:\n",
      "\tget_image_uri(region, 'xgboost', '0.90-1').\n"
     ]
    }
   ],
   "source": [
    "# Launch Training and HyperParameter Tuning Jobs using Spot Instances\n",
    "\n",
    "xgb   = Estimator(\n",
    "                  image_name               = get_image_uri(\n",
    "                                                           region_name  = boto3.Session().region_name,\n",
    "                                                           repo_name    = repo_name                  ,\n",
    "                                                           repo_version = repo_version\n",
    "                                                          )                            ,\n",
    "                  role                     = get_execution_role()                      ,\n",
    "                  train_instance_count     = 1                                         ,\n",
    "                  train_instance_type      = 'ml.m5.large'                             ,\n",
    "                  train_use_spot_instances = True                                      ,\n",
    "                  train_max_run            = 3600                                      ,\n",
    "                  train_max_wait           = 3600                                      ,\n",
    "                  output_path              = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "                 )\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "                        scale_pos_weight = class_weights    , # Calculated in previous cell\n",
    "                        objective        = 'reg:logistic'   ,\n",
    "                        eval_metric      = 'auc'            ,\n",
    "                        max_depth        = 10               ,\n",
    "                        eta              = 0.713            ,\n",
    "                        gamma            = 4                ,\n",
    "                        min_child_weight = 9.93             ,\n",
    "                        subsample        = 0.8              ,\n",
    "                        silent           = 0                ,\n",
    "                        num_round        = 100\n",
    "                       )\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "                            estimator             = xgb             ,\n",
    "                            max_jobs              = 3               ,\n",
    "                            max_parallel_jobs     = 2               ,\n",
    "                            objective_metric_name = 'validation:auc',\n",
    "                            strategy              = 'Bayesian'      ,\n",
    "                            hyperparameter_ranges = {\n",
    "                                                     'alpha'             : ContinuousParameter(0  , 1000),\n",
    "                                                     'colsample_bylevel' : ContinuousParameter(0.1,    1),\n",
    "                                                     'colsample_bytree'  : ContinuousParameter(0.5,    1),\n",
    "                                                     'lambda'            : ContinuousParameter(0  , 1000),\n",
    "                                                     'max_delta_step'    : IntegerParameter(   0  ,   10)\n",
    "                                                    }\n",
    "                           )\n",
    "\n",
    "tuner.fit({\n",
    "           'train'      : s3_input(s3_data='s3://{}/{}/train/'.format(     bucket, prefix), content_type='text/csv'),\n",
    "           'validation' : s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='text/csv')\n",
    "         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image.To use the newer image, please set 'repo_version'='0.90-1. For example:\n",
      "\tget_image_uri(region, 'xgboost', '0.90-1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------! \n",
      "Accuracy  = 99.5\n",
      "Precision = 19.4\n",
      "Recall    = 84.2\n",
      "F1 Score  = 31.5\n"
     ]
    }
   ],
   "source": [
    "# Create Ephemeral Endpoint to Analyze Best Tuned Model\n",
    "\n",
    "if sagemaker_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner.latest_tuning_job.name)['HyperParameterTuningJobStatus'] == 'Completed':\n",
    "    \n",
    "    best_training_job = sagemaker_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner.latest_tuning_job.name)['BestTrainingJob']['TrainingJobName']\n",
    "    \n",
    "    Model(\n",
    "          model_data = sagemaker_client.describe_training_job(TrainingJobName=best_training_job)['ModelArtifacts']['S3ModelArtifacts'],\n",
    "          image      = get_image_uri(\n",
    "                                     region_name  = boto3.Session().region_name,\n",
    "                                     repo_name    = repo_name                  ,\n",
    "                                     repo_version = repo_version\n",
    "                                    ),\n",
    "          role       = role\n",
    "         ).deploy(\n",
    "                  initial_instance_count = 1              ,\n",
    "                  instance_type          = 'ml.t2.2xlarge',\n",
    "                  endpoint_name          = endpoint_name\n",
    "                 )\n",
    "\n",
    "    predictor = RealTimePredictor(endpoint_name)\n",
    "\n",
    "    predictor.content_type = 'text/csv'\n",
    "    predictor.serializer   = csv_serializer\n",
    "    predictor.deserializer = json_deserializer\n",
    "\n",
    "    true_positive  = 0\n",
    "    false_positive = 0\n",
    "    true_negative  = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    for testdata in test_data.to_numpy():\n",
    "        \n",
    "        prediction = np.round(predictor.predict(testdata[1:]))\n",
    "\n",
    "        if prediction > 0:\n",
    "\n",
    "            if testdata[0] == prediction:\n",
    "                true_positive  += 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            if testdata[0] == prediction:\n",
    "                true_negative  += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "\n",
    "    predictor.delete_endpoint()\n",
    "    predictor.delete_model()\n",
    "\n",
    "    print(' ')\n",
    "    if true_positive+true_negative+false_positive+false_negative > 0:     print('Accuracy  = {:.1f}'.format(((true_positive+true_negative)/(true_positive+true_negative+false_positive+false_negative))*100))\n",
    "    if true_positive+false_positive > 0:                                  print('Precision = {:.1f}'.format((true_positive/(true_positive+false_positive))*100))\n",
    "    if true_positive+false_negative > 0:                                  print('Recall    = {:.1f}'.format((true_positive/(true_positive+false_negative))*100))\n",
    "    if true_positive+false_positive and true_positive+false_negative > 0: print('F1 Score  = {:.1f}'.format((2*((true_positive/(true_positive+false_positive))*(true_positive/(true_positive+false_negative)))/((true_positive/(true_positive+false_positive))+(true_positive/(true_positive+false_negative))))*100))\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Please wait for tuning job to complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
